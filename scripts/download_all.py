"""é€è‚¡ç²¾å‡†å¢é‡ä¸‹è½½æ²ªæ·± A è‚¡å†å² K çº¿ + è´¢åŠ¡æ•°æ®ã€‚

åŸºäºæœ¬åœ°ç¼“å­˜æ¢æµ‹ï¼Œæ¯åªè‚¡ç¥¨ä»å„è‡ªçš„æœ€æ–°ç¼“å­˜æ—¥æœŸå¼€å§‹å¢é‡ä¸‹è½½ã€‚
é¦–æ¬¡è¿è¡Œè‡ªåŠ¨å…¨é‡ï¼Œåç»­è¿è¡Œè‡ªåŠ¨ç²¾å‡†å¢é‡ã€‚

ç”¨æ³•:
    python scripts/download_all.py [OPTIONS]
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
from xtquant import xtdata

# future.result(timeout=N) åœ¨ Windows ä¸Šä¼šé•¿æ—¶é—´é˜»å¡ä¸»çº¿ç¨‹å¯¼è‡´ Ctrl+C æ— å“åº”ï¼Œ
# æ”¹ç”¨çŸ­è½®è¯¢è®© Python æ¯éš” POLL_INTERVAL ç§’æœ‰æœºä¼šå¤„ç† KeyboardInterruptã€‚
POLL_INTERVAL = 0.5

try:
    from tqdm import tqdm
except ImportError:
    print("é”™è¯¯: éœ€è¦ tqdm ä¾èµ–ï¼Œè¯·å…ˆæ‰§è¡Œ: pip install tqdm>=4.60")
    print("  æˆ–: pip install -e \".[scripts]\"")
    sys.exit(1)

# åˆ†é’Ÿçº§å‘¨æœŸæ•°æ®é‡è¿œå¤§äºæ—¥çº¿ï¼Œéœ€è¦æ›´é•¿è¶…æ—¶ï¼›ä¹˜ä»¥åŸºå‡† --timeout
PERIOD_TIMEOUT_SCALE: dict[str, float] = {
    "1m": 3.0,
    "5m": 2.5,
    "15m": 2.0,
    "30m": 1.5,
    "60m": 1.5,
}

# â”€â”€ é»˜è®¤ä¸‹è½½æ¿å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# xtdata.get_sector_list() è¿”å›çš„å¸‚åœºæ¿å—ï¼ˆå…± 30 ä¸ªï¼Œä¸å« SW/CSRC è¡Œä¸šåˆ†ç±»ï¼‰:
#
# è‚¡ç¥¨:
#   æ²ªæ·±Aè‚¡ (5189)    = ä¸Šè¯Aè‚¡ (2306) + æ·±è¯Aè‚¡ (2883)ï¼Œå«åˆ›ä¸šæ¿ (1392) å’Œç§‘åˆ›æ¿ (602)
#   æ²ªæ·±äº¬Aè‚¡ (5501)  = æ²ªæ·±Aè‚¡ + äº¬å¸‚Aè‚¡ (312)
#   æ²ªæ·±Bè‚¡ (79)      = ä¸Šè¯Bè‚¡ (41) + æ·±è¯Bè‚¡ (38)
#   ç§‘åˆ›æ¿CDR (1)
#
# ETF:
#   æ²ªæ·±ETF (1460)    = æ²ªå¸‚ETF (855) + æ·±å¸‚ETF (605)
#
# æŒ‡æ•°:
#   æ²ªæ·±æŒ‡æ•° (609)    = æ²ªå¸‚æŒ‡æ•° (221) + æ·±å¸‚æŒ‡æ•° (388)
#
# è½¬å€º:
#   æ²ªæ·±è½¬å€º (383)    = ä¸Šè¯è½¬å€º (187) + æ·±è¯è½¬å€º (196)
#
# å€ºåˆ¸:
#   æ²ªæ·±å€ºåˆ¸ (39583)  = æ²ªå¸‚å€ºåˆ¸ (21268) + æ·±å¸‚å€ºåˆ¸ (18315)
#
# åŸºé‡‘:
#   æ²ªæ·±åŸºé‡‘ (2004)   = æ²ªå¸‚åŸºé‡‘ (1027) + æ·±å¸‚åŸºé‡‘ (977)
#
# æœŸæƒ:
#   ä¸Šè¯æœŸæƒ (760), æ·±è¯æœŸæƒ (616)
#
# æ¸¯è‚¡:
#   é¦™æ¸¯è”äº¤æ‰€è‚¡ç¥¨ (882), é¦™æ¸¯è”äº¤æ‰€æŒ‡æ•° (0)
#
DEFAULT_SECTORS = "æ²ªæ·±Aè‚¡,æ²ªæ·±ETF,æ²ªæ·±æŒ‡æ•°,æ²ªæ·±è½¬å€º"

# â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROBE_BATCH_SIZE = 200
SAFETY_OVERLAP_DAYS = 1

# Ctrl+C ä¸­æ–­æ ‡è®°ï¼šxtdata ä¸‹è½½çº¿ç¨‹æ˜¯é daemon çº¿ç¨‹ï¼Œ
# å³ä½¿ executor.shutdown(wait=False) ä¹Ÿæ— æ³•ç»ˆæ­¢å·²è¿è¡Œçš„çº¿ç¨‹ï¼Œ
# Python é€€å‡ºæ—¶ä¼šç­‰å¾…è¿™äº›çº¿ç¨‹å®Œæˆå¯¼è‡´å¡æ­»ã€‚
# è®¾ç½®æ­¤æ ‡è®°å main() ç»“æŸæ—¶ç”¨ os._exit(0) å¼ºåˆ¶é€€å‡ºã€‚
_interrupted = False

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOG_DIR = Path(__file__).resolve().parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logger = logging.getLogger("download_all")
logger.setLevel(logging.DEBUG)

# æ–‡ä»¶ handlerï¼šè¯¦ç»†æ—¥å¿—å†™å…¥ logs/download_all_<date>.log
_log_file = LOG_DIR / f"download_all_{datetime.now():%Y%m%d_%H%M%S}.log"
_fh = logging.FileHandler(_log_file, encoding="utf-8")
_fh.setLevel(logging.DEBUG)
_fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)-7s %(message)s"))
logger.addHandler(_fh)

# æ§åˆ¶å° handlerï¼šä»… WARNING ä»¥ä¸Šï¼ˆé¿å…ä¸ tqdm å†²çªï¼‰
_ch = logging.StreamHandler()
_ch.setLevel(logging.WARNING)
_ch.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
logger.addHandler(_ch)

# â”€â”€ çŠ¶æ€æŒä¹…åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STATE_FILE = LOG_DIR / "download_state.json"
STATE_VERSION = 1


@dataclass
class TaskState:
    """å•ä¸ªä¸‹è½½ä»»åŠ¡çš„çŠ¶æ€ã€‚"""
    last_success_date: str = ""
    last_run_iso: str = ""
    stock_count: int = 0
    ok: int = 0
    fail: int = 0


@dataclass
class DownloadState:
    """å…¨å±€ä¸‹è½½çŠ¶æ€å®¹å™¨ã€‚"""
    version: int = STATE_VERSION
    tasks: dict[str, TaskState] = field(default_factory=dict)


def load_state() -> DownloadState:
    """è¯»å–çŠ¶æ€æ–‡ä»¶ï¼Œå¼‚å¸¸æ—¶å›é€€ç©ºçŠ¶æ€ã€‚"""
    if not STATE_FILE.exists():
        return DownloadState()
    try:
        raw = json.loads(STATE_FILE.read_text(encoding="utf-8"))
        state = DownloadState(version=raw.get("version", STATE_VERSION))
        for key, val in raw.get("tasks", {}).items():
            state.tasks[key] = TaskState(**val)
        return state
    except Exception as exc:
        logger.warning("è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨ç©ºçŠ¶æ€: %s", exc)
        return DownloadState()


def save_state(state: DownloadState) -> None:
    """å°†çŠ¶æ€å†™å…¥ JSON æ–‡ä»¶ã€‚"""
    data = {
        "version": state.version,
        "tasks": {k: asdict(v) for k, v in state.tasks.items()},
    }
    STATE_FILE.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
    logger.info("çŠ¶æ€å·²ä¿å­˜: %s", STATE_FILE)


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_batches(lst: list, size: int) -> list[list]:
    """å°†åˆ—è¡¨æŒ‰ size åˆ‡åˆ†ä¸ºå­åˆ—è¡¨ã€‚"""
    return [lst[i : i + size] for i in range(0, len(lst), size)]


def _make_kline_cb(
    flag: list[bool], codes: list[str], fail_count: int, timeout_count: int, pbar: tqdm,
) -> callable:
    """åˆ›å»º K çº¿ä¸‹è½½å›è°ƒï¼Œç”¨äºæ›´æ–° tqdm è¿›åº¦æ¡ã€‚"""
    n_codes = len(codes)
    def _on_progress(data: dict) -> None:
        if flag[0]:
            return
        finished = data.get("finished", 0)
        total = data.get("total", 0)
        if total > 0:
            stock_idx = min(int(finished * n_codes / total), n_codes) - 1
        else:
            stock_idx = -1
        parts = [f"æ‰¹å†… {finished}/{total}"]
        if 0 <= stock_idx < n_codes:
            parts.append(codes[stock_idx])
        if fail_count or timeout_count:
            parts.append(f"å¤±è´¥:{fail_count} è¶…æ—¶:{timeout_count}")
        pbar.set_postfix_str(" | ".join(parts), refresh=True)
    return _on_progress


def _wait_future(future, timeout: float) -> None:
    """ç­‰å¾… future å®Œæˆï¼Œæ¯ POLL_INTERVAL ç§’é†’æ¥æ£€æŸ¥ KeyboardInterruptã€‚

    åœ¨ Windows ä¸Š future.result(timeout=N) ä¼šé•¿æ—¶é—´é˜»å¡ä¸»çº¿ç¨‹ï¼Œ
    Ctrl+C ä¿¡å·è¦ç­‰åˆ° timeout åˆ°æœŸæ‰èƒ½è¢«å¤„ç†ã€‚
    è¿™é‡Œç”¨çŸ­è½®è¯¢æ›¿ä»£ï¼Œç¡®ä¿ Ctrl+C èƒ½åœ¨ 0.5 ç§’å†…å“åº”ã€‚
    """
    deadline = time.monotonic() + timeout
    while True:
        remaining = deadline - time.monotonic()
        if remaining <= 0:
            raise FutureTimeoutError()
        try:
            future.result(timeout=min(remaining, POLL_INTERVAL))
            return  # æˆåŠŸå®Œæˆ
        except FutureTimeoutError:
            if time.monotonic() >= deadline:
                raise
            # æœªè¶…æ—¶ï¼Œç»§ç»­è½®è¯¢ï¼ˆæ­¤å¤„ KeyboardInterrupt å¯è¢«æ•è·ï¼‰


def _run_kline_batches(
    batches: list[list[str]],
    batch_indices: list[int],
    period: str,
    start_time: str,
    timeout: int,
    delay: float,
    pbar: tqdm,
    label: str,
) -> tuple[int, int, int, list[int], bool]:
    """æ‰§è¡Œä¸€è½® K çº¿æ‰¹æ¬¡ä¸‹è½½ã€‚

    Returns:
        (ok_count, fail_count, timeout_count, failed_indices, interrupted)
    """
    ok_count = 0
    fail_count = 0
    timeout_count = 0
    failed_indices: list[int] = []
    n_total = len(batch_indices)

    for seq, idx in enumerate(batch_indices):
        batch = batches[idx]
        cancelled = [False]
        pbar.set_description(f"{label} [{seq+1}/{n_total}æ‰¹]")

        executor = ThreadPoolExecutor(max_workers=1)
        try:
            future = executor.submit(
                xtdata.download_history_data2,
                stock_list=batch,
                period=period,
                start_time=start_time,
                end_time="",
                callback=_make_kline_cb(cancelled, batch, fail_count, timeout_count, pbar),
                incrementally=True,
            )
            _wait_future(future, timeout)
            ok_count += len(batch)
            logger.debug("Kçº¿ %s æ‰¹æ¬¡ %d æˆåŠŸ (%d åª)", period, idx+1, len(batch))
        except FutureTimeoutError:
            cancelled[0] = True
            timeout_count += 1
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("Kçº¿ %s æ‰¹æ¬¡ %d è¶…æ—¶ (%dç§’, %d åª)", period, idx+1, timeout, len(batch))
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} è¶…æ—¶ ({timeout}s, {len(batch)} åª)")
        except KeyboardInterrupt:
            global _interrupted
            _interrupted = True
            cancelled[0] = True
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.close()
            logger.warning("Kçº¿ %s è¢«ç”¨æˆ·ä¸­æ–­", period)
            tqdm.write(f"\n  ç”¨æˆ·ä¸­æ–­ï¼ŒKçº¿ {period} æœ¬è½®å·²å®Œæˆ {ok_count} åª")
            return ok_count, fail_count, timeout_count, failed_indices, True
        except Exception as exc:
            cancelled[0] = True
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("Kçº¿ %s æ‰¹æ¬¡ %d å¤±è´¥ (%d åª): %s", period, idx+1, len(batch), exc)
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} å¤±è´¥ ({len(batch)} åª): {exc}")
        finally:
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.update(len(batch))

        if delay > 0 and seq < n_total - 1:
            time.sleep(delay)
    else:
        pbar.close()

    return ok_count, fail_count, timeout_count, failed_indices, False


def _make_financial_cb(
    flag: list[bool], codes: list[str], tables: list[str],
    fail_count: int, timeout_count: int, pbar: tqdm,
) -> callable:
    """åˆ›å»ºè´¢åŠ¡æ•°æ®ä¸‹è½½å›è°ƒï¼Œç”¨äºæ›´æ–° tqdm è¿›åº¦æ¡ã€‚"""
    n_codes = len(codes)
    n_tables = len(tables)
    def _on_progress(data: dict) -> None:
        if flag[0]:
            return
        finished = data.get("finished", 0)
        total = data.get("total", 0)
        parts = [f"æ‰¹å†… {finished}/{total}"]
        if total > 0:
            item_est = min(int(finished * n_codes * n_tables / total), n_codes * n_tables) - 1
            if item_est >= 0:
                stock_idx = item_est // n_tables
                table_idx = item_est % n_tables
                if stock_idx < n_codes:
                    parts.append(f"{codes[stock_idx]}/{tables[table_idx]}")
        if fail_count or timeout_count:
            parts.append(f"å¤±è´¥:{fail_count} è¶…æ—¶:{timeout_count}")
        pbar.set_postfix_str(" | ".join(parts), refresh=True)
    return _on_progress


def _run_financial_batches(
    batches: list[list[str]],
    batch_indices: list[int],
    table_list: list[str],
    timeout: int,
    delay: float,
    pbar: tqdm,
    label: str,
) -> tuple[int, int, int, list[int], bool]:
    """æ‰§è¡Œä¸€è½®è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ä¸‹è½½ã€‚

    Returns:
        (ok_count, fail_count, timeout_count, failed_indices, interrupted)
    """
    ok_count = 0
    fail_count = 0
    timeout_count = 0
    failed_indices: list[int] = []
    n_total = len(batch_indices)

    for seq, idx in enumerate(batch_indices):
        batch = batches[idx]
        batch_items = len(batch) * len(table_list)
        cancelled = [False]
        pbar.set_description(f"{label} [{seq+1}/{n_total}æ‰¹]")

        executor = ThreadPoolExecutor(max_workers=1)
        try:
            future = executor.submit(
                xtdata.download_financial_data2,
                stock_list=batch,
                table_list=table_list,
                callback=_make_financial_cb(cancelled, batch, table_list, fail_count, timeout_count, pbar),
            )
            _wait_future(future, timeout)
            ok_count += len(batch)
            logger.debug("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d æˆåŠŸ (%d åª)", idx+1, len(batch))
        except FutureTimeoutError:
            cancelled[0] = True
            timeout_count += 1
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d è¶…æ—¶ (%dç§’, %d åª)", idx+1, timeout, len(batch))
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} è¶…æ—¶ ({timeout}s, {len(batch)} åª)")
        except KeyboardInterrupt:
            global _interrupted  # noqa: PLW0602 (already declared in kline handler)
            _interrupted = True
            cancelled[0] = True
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.close()
            logger.warning("è´¢åŠ¡æ•°æ®è¢«ç”¨æˆ·ä¸­æ–­")
            tqdm.write(f"\n  ç”¨æˆ·ä¸­æ–­ï¼Œè´¢åŠ¡æ•°æ®æœ¬è½®å·²å®Œæˆ {ok_count} åª")
            return ok_count, fail_count, timeout_count, failed_indices, True
        except Exception as exc:
            cancelled[0] = True
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d å¤±è´¥ (%d åª): %s", idx+1, len(batch), exc)
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} å¤±è´¥ ({len(batch)} åª): {exc}")
        finally:
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.update(batch_items)

        if delay > 0 and seq < n_total - 1:
            time.sleep(delay)
    else:
        pbar.close()

    return ok_count, fail_count, timeout_count, failed_indices, False


def download_financial(
    stocks: list[str],
    table_list: list[str],
    batch_size: int,
    timeout: int = 120,
    delay: float = 0.2,
    max_retries: int = 2,
) -> dict[str, int]:
    """ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼Œè¿”å› {"ok": n, "fail": n, "timeout": n}ã€‚

    é€šè¿‡ callback å®ç°é€é¡¹ï¼ˆè‚¡ç¥¨ Ã— æŠ¥è¡¨ï¼‰ç²’åº¦çš„è¿›åº¦æ›´æ–°ã€‚
    æ¯æ‰¹ä¸‹è½½æœ‰è¶…æ—¶ä¿æŠ¤ï¼Œæ‰¹æ¬¡é—´æœ‰å»¶è¿Ÿä»¥ç¼“è§£æœåŠ¡ç«¯å‹åŠ›ã€‚
    è¶…æ—¶å¤±è´¥çš„æ‰¹æ¬¡ä¼šè‡ªåŠ¨é‡è¯•ï¼Œæ¯è½®é‡è¯•è¶…æ—¶å†å¢åŠ  50%ã€‚
    """
    batches = make_batches(stocks, batch_size)
    total_items = len(stocks) * len(table_list)
    n_batches = len(batches)
    all_indices = list(range(n_batches))

    logger.info(
        "å¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼Œå…± %d æ‰¹ (%d åª Ã— %d è¡¨ = %d é¡¹)",
        n_batches, len(stocks), len(table_list), total_items,
    )
    pbar = tqdm(total=total_items, desc="è´¢åŠ¡", unit="é¡¹")

    # â”€â”€ é¦–è½®ä¸‹è½½ â”€â”€
    ok, fail, to, failed, interrupted = _run_financial_batches(
        batches, all_indices, table_list, timeout, delay, pbar, "è´¢åŠ¡",
    )

    # â”€â”€ è‡ªåŠ¨é‡è¯•å¤±è´¥æ‰¹æ¬¡ â”€â”€
    for retry_round in range(1, max_retries + 1):
        if not failed or interrupted:
            break
        retry_timeout = int(timeout * (1.5 ** retry_round))
        n_retry = len(failed)
        retry_stocks = sum(len(batches[i]) for i in failed)
        retry_items = retry_stocks * len(table_list)
        tqdm.write(
            f"  ğŸ”„ è´¢åŠ¡æ•°æ®é‡è¯•ç¬¬ {retry_round}/{max_retries} è½®: "
            f"{n_retry} ä¸ªæ‰¹æ¬¡ ({retry_stocks} åª), è¶…æ—¶ {retry_timeout}s"
        )
        logger.info(
            "è´¢åŠ¡æ•°æ®é‡è¯•ç¬¬ %d è½®: %d ä¸ªæ‰¹æ¬¡ (%d åª), è¶…æ—¶ %ds",
            retry_round, n_retry, retry_stocks, retry_timeout,
        )
        retry_pbar = tqdm(total=retry_items, desc=f"è´¢åŠ¡ é‡è¯•{retry_round}", unit="é¡¹")
        r_ok, r_fail, r_to, still_failed, interrupted = _run_financial_batches(
            batches, failed, table_list, retry_timeout, delay, retry_pbar, f"è´¢åŠ¡ é‡è¯•{retry_round}",
        )
        ok += r_ok
        failed = still_failed

    # æœ€ç»ˆä¿®æ­£è®¡æ•°
    final_fail_stocks = sum(len(batches[i]) for i in failed)
    ok = len(stocks) - final_fail_stocks if not interrupted else ok
    fail = final_fail_stocks
    to = len(failed)

    logger.info("è´¢åŠ¡æ•°æ®å®Œæˆ: æˆåŠŸ %d, å¤±è´¥ %d (å…¶ä¸­è¶…æ—¶ %d)", ok, fail, to)
    if failed:
        logger.warning("è´¢åŠ¡æ•°æ®æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: %s", failed)
        tqdm.write(f"  è´¢åŠ¡æ•°æ®æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: {failed}")

    return {"ok": ok, "fail": fail, "timeout": to}


# â”€â”€ v2 æ–°å¢ï¼šç¼“å­˜æ¢æµ‹ä¸åˆ†ç»„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def probe_local_dates(stocks: list[str], period: str) -> dict[str, str]:
    """æ‰¹é‡æ¢æµ‹æ¯åªè‚¡ç¥¨æœ¬åœ°ç¼“å­˜çš„æœ€æ–°æ•°æ®æ—¥æœŸã€‚

    å¯¹å…¨éƒ¨è‚¡ç¥¨åˆ†æ‰¹è°ƒç”¨ get_local_data(count=1)ï¼Œ
    æ¯æ‰¹ 200 åªï¼Œæ¯åªä»…è¿”å›æœ€å 1 æ¡è®°å½•ã€‚

    Returns:
        {stock_code: "YYYYMMDD"} â€” æ— æœ¬åœ°æ•°æ®çš„è‚¡ç¥¨ä¸åœ¨å­—å…¸ä¸­ã€‚
    """
    result: dict[str, str] = {}
    probe_pbar = tqdm(total=len(stocks), desc="æ¢æµ‹æœ¬åœ°ç¼“å­˜", unit="åª")
    for i in range(0, len(stocks), PROBE_BATCH_SIZE):
        batch = stocks[i : i + PROBE_BATCH_SIZE]
        try:
            data = xtdata.get_local_data(
                field_list=[], stock_list=batch,
                period=period, start_time="", end_time="", count=1,
            )
            for stock, df in data.items():
                if df is not None and not df.empty:
                    last_ts = df.index[-1]
                    if isinstance(last_ts, (int, float)):
                        dt = datetime.fromtimestamp(last_ts / 1000)
                    else:
                        dt = pd.Timestamp(last_ts).to_pydatetime()
                    result[stock] = dt.strftime("%Y%m%d")
        except Exception as exc:
            logger.warning("ç¼“å­˜æ¢æµ‹æ‰¹æ¬¡å¤±è´¥: %s", exc)
        probe_pbar.update(len(batch))
    probe_pbar.close()
    return result


def group_stocks_by_date(
    stocks: list[str],
    local_dates: dict[str, str],
) -> list[tuple[str, list[str]]]:
    """æŒ‰æœ¬åœ°ç¼“å­˜æœ€æ–°æ—¥æœŸåˆ†ç»„ã€‚

    æœ‰ç¼“å­˜çš„è‚¡ç¥¨: start_time = last_date - SAFETY_OVERLAP_DAYS
    æ— ç¼“å­˜çš„è‚¡ç¥¨: start_time = "" (å…¨é‡)

    Returns:
        [(start_time, [stock_codes]), ...] æŒ‰ start_time æ’åºï¼ˆ""åœ¨æœ€å‰ï¼‰ã€‚
    """
    groups: dict[str, list[str]] = defaultdict(list)
    for stock in stocks:
        last_date = local_dates.get(stock)
        if last_date:
            overlap_dt = datetime.strptime(last_date, "%Y%m%d") - timedelta(days=SAFETY_OVERLAP_DAYS)
            groups[overlap_dt.strftime("%Y%m%d")].append(stock)
        else:
            groups[""].append(stock)
    return sorted(groups.items(), key=lambda x: x[0])


# â”€â”€ v2 æ–°å¢ï¼šåˆ†ç»„ä¸‹è½½ä¸»å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_kline_v2(
    stocks: list[str],
    periods: list[str],
    full: bool,
    batch_size: int,
    timeout: int,
    delay: float,
    max_retries: int,
) -> dict[str, dict[str, int]]:
    """æŒ‰é€è‚¡ç²¾å‡†å¢é‡ç­–ç•¥ä¸‹è½½ K çº¿æ•°æ®ã€‚

    --full æ¨¡å¼: æ‰€æœ‰è‚¡ç¥¨ç»Ÿä¸€ start_time=""
    é»˜è®¤æ¨¡å¼: å¯¹æ¯ä¸ª period æ¢æµ‹æœ¬åœ°ç¼“å­˜ï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„åæ‰¹é‡ä¸‹è½½ã€‚

    Returns:
        {period: {"ok": n, "fail": n, "timeout": n, "date_groups": n}}
    """
    results: dict[str, dict[str, int]] = {}

    for period in periods:
        scale = PERIOD_TIMEOUT_SCALE.get(period, 1.0)
        effective_timeout = int(timeout * scale)
        if scale > 1.0:
            tqdm.write(f"  å‘¨æœŸ {period} è¶…æ—¶è‡ªåŠ¨è°ƒæ•´: {timeout}s Ã— {scale} = {effective_timeout}s")
            logger.info("Kçº¿ %s è¶…æ—¶è°ƒæ•´: %d Ã— %.1f = %d", period, timeout, scale, effective_timeout)

        if full:
            # --full: æ‰€æœ‰è‚¡ç¥¨ç»Ÿä¸€å…¨é‡
            date_groups = [("", stocks)]
        else:
            # é»˜è®¤: é€è‚¡ç²¾å‡†å¢é‡
            tqdm.write(f"\næ¢æµ‹ {period} æœ¬åœ°ç¼“å­˜...")
            local_dates = probe_local_dates(stocks, period)
            date_groups = group_stocks_by_date(stocks, local_dates)
            # æ‰“å°åˆ†ç»„æ‘˜è¦
            for st, grp in date_groups:
                label = f"èµ·å§‹ {st}" if st else "å…¨é‡ (æ— æœ¬åœ°ç¼“å­˜)"
                tqdm.write(f"  Â· {len(grp)} åª â†’ {label}")

        n_date_groups = len(date_groups)

        # æŒ‰ç»„ä¸‹è½½
        total_stocks = sum(len(g) for _, g in date_groups)
        pbar = tqdm(total=total_stocks, desc=f"Kçº¿ {period}", unit="åª")
        total_ok = 0
        total_fail = 0
        total_to = 0
        interrupted = False

        for start_time, group_stocks in date_groups:
            batches = make_batches(group_stocks, batch_size)
            all_indices = list(range(len(batches)))
            n_batches = len(batches)
            st_label = start_time or "(å…¨é‡)"
            logger.info(
                "å¼€å§‹ä¸‹è½½ K çº¿ %sï¼Œç»„ start=%sï¼Œå…± %d æ‰¹ (%d åª), è¶…æ—¶ %ds",
                period, st_label, n_batches, len(group_stocks), effective_timeout,
            )

            ok, fail, to, failed, interrupted = _run_kline_batches(
                batches, all_indices, period, start_time,
                effective_timeout, delay, pbar, f"Kçº¿ {period}",
            )

            # è‡ªåŠ¨é‡è¯•å¤±è´¥æ‰¹æ¬¡
            for retry_round in range(1, max_retries + 1):
                if not failed or interrupted:
                    break
                retry_timeout = int(effective_timeout * (1.5 ** retry_round))
                n_retry = len(failed)
                retry_stocks = sum(len(batches[i]) for i in failed)
                tqdm.write(
                    f"  ğŸ”„ Kçº¿ {period} é‡è¯•ç¬¬ {retry_round}/{max_retries} è½®: "
                    f"{n_retry} ä¸ªæ‰¹æ¬¡ ({retry_stocks} åª), è¶…æ—¶ {retry_timeout}s"
                )
                logger.info(
                    "Kçº¿ %s é‡è¯•ç¬¬ %d è½®: %d ä¸ªæ‰¹æ¬¡ (%d åª), è¶…æ—¶ %ds",
                    period, retry_round, n_retry, retry_stocks, retry_timeout,
                )
                retry_pbar = tqdm(total=retry_stocks, desc=f"Kçº¿ {period} é‡è¯•{retry_round}", unit="åª")
                r_ok, r_fail, r_to, still_failed, interrupted = _run_kline_batches(
                    batches, failed, period, start_time, retry_timeout, delay, retry_pbar,
                    f"Kçº¿ {period} é‡è¯•{retry_round}",
                )
                ok += r_ok
                failed = still_failed

            final_fail = sum(len(batches[i]) for i in failed)
            ok = len(group_stocks) - final_fail if not interrupted else ok
            total_ok += ok
            total_fail += final_fail
            total_to += len(failed)

            if failed:
                logger.warning("Kçº¿ %s (start=%s) æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: %s", period, st_label, failed)
                tqdm.write(f"  {period} (start={st_label}) æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: {failed}")

            if interrupted:
                break

        pbar.close()
        results[period] = {
            "ok": total_ok, "fail": total_fail, "timeout": total_to,
            "date_groups": n_date_groups,
        }
        logger.info(
            "Kçº¿ %s å®Œæˆ: æˆåŠŸ %d, å¤±è´¥ %d (è¶…æ—¶ %d), æ—¥æœŸç»„ %d",
            period, total_ok, total_fail, total_to, n_date_groups,
        )
        if interrupted:
            break

    return results


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="é€è‚¡ç²¾å‡†å¢é‡ä¸‹è½½æ²ªæ·± A è‚¡å†å²è¡Œæƒ… + è´¢åŠ¡æ•°æ® (v2)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        default=False,
        help="å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼ˆè·³è¿‡ç¼“å­˜æ¢æµ‹ï¼Œæ‰€æœ‰è‚¡ç¥¨ start_time=\"\"ï¼‰",
    )
    parser.add_argument(
        "--periods",
        default="1d,5m,1m",
        help="K çº¿å‘¨æœŸï¼Œé€—å·åˆ†éš” (é»˜è®¤: 1d,5m,1m)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="æ¯æ‰¹è‚¡ç¥¨æ•°é‡ (é»˜è®¤: 50)",
    )
    parser.add_argument(
        "--tables",
        default="Balance,Income,CashFlow",
        help="è´¢åŠ¡æŠ¥è¡¨ç±»å‹ï¼Œé€—å·åˆ†éš” (é»˜è®¤: Balance,Income,CashFlow)",
    )
    parser.add_argument(
        "--skip-kline",
        action="store_true",
        help="è·³è¿‡ K çº¿ä¸‹è½½",
    )
    parser.add_argument(
        "--skip-financial",
        action="store_true",
        help="è·³è¿‡è´¢åŠ¡æ•°æ®ä¸‹è½½",
    )
    parser.add_argument(
        "--sectors",
        default=DEFAULT_SECTORS,
        help=f"ç›®æ ‡æ¿å—ï¼Œé€—å·åˆ†éš” (é»˜è®¤: {DEFAULT_SECTORS})",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=120,
        help="æ¯æ‰¹ä¸‹è½½è¶…æ—¶ç§’æ•° (é»˜è®¤: 120)",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.2,
        help="æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•°ï¼Œç¼“è§£æœåŠ¡ç«¯å‹åŠ› (é»˜è®¤: 0.2)",
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=2,
        help="è¶…æ—¶æ‰¹æ¬¡æœ€å¤§è‡ªåŠ¨é‡è¯•æ¬¡æ•° (é»˜è®¤: 2)",
    )
    return parser.parse_args()


def print_summary(
    total: int,
    elapsed: float,
    kline_results: dict[str, dict[str, int]] | None,
    financial_result: dict[str, int] | None,
    full: bool,
    state_saved: bool = False,
) -> None:
    """æ‰“å°ä¸‹è½½ç»“æœæ±‡æ€»ï¼ˆå«æ¢æµ‹åˆ†ç»„ä¿¡æ¯ï¼‰ã€‚"""
    minutes = elapsed / 60
    has_failure = False

    print()
    print("=" * 60)
    print("ä¸‹è½½å®Œæˆ â€” ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"è‚¡ç¥¨æ€»æ•°: {total}")
    print(f"è€—æ—¶: {elapsed:.1f} ç§’ ({minutes:.1f} åˆ†é’Ÿ)")

    if kline_results:
        print()
        print("Kçº¿æ•°æ®:")
        for period, counts in kline_results.items():
            n_groups = counts.get("date_groups", 0)
            if full:
                mode_info = "å…¨é‡"
            elif n_groups > 0:
                mode_info = f"ç²¾å‡†å¢é‡: {n_groups} ä¸ªæ—¥æœŸç»„"
            else:
                mode_info = ""
            if counts["fail"] == 0:
                print(f"  {period}: æˆåŠŸ {counts['ok']}, OK ({mode_info})")
            else:
                has_failure = True
                timeout_info = f" (è¶…æ—¶ {counts['timeout']})" if counts.get("timeout") else ""
                print(f"  {period}: æˆåŠŸ {counts['ok']}, å¤±è´¥ {counts['fail']}{timeout_info} ({mode_info})")

    if financial_result:
        print()
        if financial_result["fail"] == 0:
            print(f"è´¢åŠ¡æ•°æ®: æˆåŠŸ {financial_result['ok']}, OK")
        else:
            has_failure = True
            timeout_info = f" (è¶…æ—¶ {financial_result['timeout']})" if financial_result.get("timeout") else ""
            print(
                f"è´¢åŠ¡æ•°æ®: æˆåŠŸ {financial_result['ok']}, "
                f"å¤±è´¥ {financial_result['fail']}{timeout_info}"
            )

    if has_failure:
        print()
        print("âš ï¸  éƒ¨åˆ†æ‰¹æ¬¡ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—åé‡è¯•")

    if state_saved:
        print()
        print(f"çŠ¶æ€æ–‡ä»¶: {STATE_FILE}")

    print("=" * 60)


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    args = parse_args()

    logger.info("æ—¥å¿—æ–‡ä»¶: %s", _log_file)
    print(f"æ—¥å¿—æ–‡ä»¶: {_log_file}")

    # â”€â”€ çŠ¶æ€ç®¡ç† â”€â”€
    state = load_state()

    # 1. è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ”¯æŒå¤šæ¿å—åˆå¹¶å»é‡ï¼‰
    sectors = [s.strip() for s in args.sectors.split(",")]
    stocks: list[str] = []
    seen: set[str] = set()
    for sector in sectors:
        codes = xtdata.get_stock_list_in_sector(sector)
        logger.info("æ¿å— [%s] è¿”å› %d åª", sector, len(codes))
        for c in codes:
            if c not in seen:
                seen.add(c)
                stocks.append(c)
    if not stocks:
        logger.error("æ¿å— %s è¿”å›ç©ºåˆ—è¡¨", sectors)
        print(f"é”™è¯¯: æ¿å— {sectors} è¿”å›ç©ºåˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ xtdata è¿æ¥çŠ¶æ€")
        sys.exit(1)
    print(f"æ¿å— {sectors} å…± {len(stocks)} åªæ ‡çš„")

    periods = [p.strip() for p in args.periods.split(",")]
    tables = [t.strip() for t in args.tables.split(",")]

    # æ‰“å°æ¨¡å¼ä¿¡æ¯
    if args.full:
        print("æ¨¡å¼: å¼ºåˆ¶å…¨é‡ä¸‹è½½ (--full)")
        logger.info("å¼ºåˆ¶å…¨é‡æ¨¡å¼ (--full)")
    else:
        print("æ¨¡å¼: é€è‚¡ç²¾å‡†å¢é‡ (åŸºäºæœ¬åœ°ç¼“å­˜æ¢æµ‹)")
        logger.info("é€è‚¡ç²¾å‡†å¢é‡æ¨¡å¼")

    logger.info("è¶…æ—¶: %dç§’/æ‰¹, å»¶è¿Ÿ: %.1fç§’/æ‰¹, æœ€å¤§é‡è¯•: %d", args.timeout, args.delay, args.max_retries)
    print(f"è¶…æ—¶: {args.timeout}ç§’/æ‰¹, æ‰¹æ¬¡é—´å»¶è¿Ÿ: {args.delay}ç§’, å¤±è´¥è‡ªåŠ¨é‡è¯•: {args.max_retries} è½®")

    print()
    t0 = time.time()
    kline_results = None
    financial_result = None
    today = datetime.now().strftime("%Y%m%d")
    now_iso = datetime.now().isoformat(timespec="seconds")

    try:
        # 2. K çº¿ä¸‹è½½
        if not args.skip_kline:
            print(f"å¼€å§‹ä¸‹è½½ K çº¿æ•°æ® (å‘¨æœŸ: {', '.join(periods)})...")
            kline_results = download_kline_v2(
                stocks, periods,
                full=args.full,
                batch_size=args.batch_size,
                timeout=args.timeout,
                delay=args.delay,
                max_retries=args.max_retries,
            )
        else:
            print("è·³è¿‡ K çº¿ä¸‹è½½")

        # 3. è´¢åŠ¡æ•°æ®ä¸‹è½½
        if not args.skip_financial:
            print(f"\nå¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ® (æŠ¥è¡¨: {', '.join(tables)})...")
            financial_result = download_financial(
                stocks, tables, args.batch_size,
                timeout=args.timeout, delay=args.delay, max_retries=args.max_retries,
            )
        else:
            print("è·³è¿‡è´¢åŠ¡æ•°æ®ä¸‹è½½")
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        print("\n\nç”¨æˆ·ä¸­æ–­ (Ctrl+C)")

    elapsed = time.time() - t0

    # â”€â”€ æ›´æ–°çŠ¶æ€ â”€â”€
    if kline_results:
        for period, counts in kline_results.items():
            task_key = f"kline:{period}"
            old = state.tasks.get(task_key)
            ts = TaskState(
                last_success_date=old.last_success_date if old else "",
                last_run_iso=now_iso,
                stock_count=len(stocks),
                ok=counts["ok"],
                fail=counts["fail"],
            )
            if counts["fail"] == 0:
                ts.last_success_date = today
            state.tasks[task_key] = ts
    if financial_result:
        task_key = "financial"
        old = state.tasks.get(task_key)
        ts = TaskState(
            last_success_date=old.last_success_date if old else "",
            last_run_iso=now_iso,
            stock_count=len(stocks),
            ok=financial_result["ok"],
            fail=financial_result["fail"],
        )
        if financial_result["fail"] == 0:
            ts.last_success_date = today
        state.tasks[task_key] = ts
    save_state(state)

    # 4. æ±‡æ€»ï¼ˆå³ä½¿ä¸­æ–­ä¹Ÿæ‰“å°å·²å®Œæˆçš„éƒ¨åˆ†ï¼‰
    print_summary(
        len(stocks), elapsed, kline_results, financial_result,
        full=args.full, state_saved=True,
    )
    logger.info("å®Œæˆï¼Œè€—æ—¶ %.1f ç§’", elapsed)

    # xtdata ä¸‹è½½çº¿ç¨‹æ˜¯é daemon çº¿ç¨‹ï¼Œä¸­æ–­åä»åœ¨åå°è¿è¡Œï¼Œ
    # æ­£å¸¸ sys.exit() ä¼šç­‰å¾…è¿™äº›çº¿ç¨‹å®Œæˆå¯¼è‡´å¡æ­»ï¼Œéœ€å¼ºåˆ¶é€€å‡ºã€‚
    if _interrupted:
        os._exit(0)


if __name__ == "__main__":
    main()
