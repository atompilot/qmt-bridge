"""é€è‚¡ç²¾å‡†å¢é‡ä¸‹è½½æ²ªæ·± A è‚¡å†å² K çº¿ + è´¢åŠ¡æ•°æ®ã€‚

åŸºäºæœ¬åœ°ç¼“å­˜æ¢æµ‹ï¼Œæ¯åªè‚¡ç¥¨ä»å„è‡ªçš„æœ€æ–°ç¼“å­˜æ—¥æœŸå¼€å§‹å¢é‡ä¸‹è½½ã€‚
é¦–æ¬¡è¿è¡Œè‡ªåŠ¨å…¨é‡ï¼Œåç»­è¿è¡Œè‡ªåŠ¨ç²¾å‡†å¢é‡ã€‚

æ”¯æŒ --since YYYY æŒ‰å¹´åº¦åˆ†æ®µä¸‹è½½ï¼Œé€‚åˆå¿«é€Ÿè·å–è¿‘æœŸæ•°æ®åé€æ­¥å›å¡«å†å²ã€‚

ç”¨æ³•:
    python scripts/download_all.py [OPTIONS]
    python scripts/download_all.py --periods 1m --skip-financial --since 2025
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
from xtquant import xtdata

# ç›´æ¥è°ƒç”¨ client.supply_history_data2() ç»•è¿‡ xtquant çš„ download_history_data2 bug:
# å½“ result=Trueï¼ˆæ•°æ®å·²ç¼“å­˜ï¼‰æ—¶ï¼Œxtquant çš„è½®è¯¢å¾ªç¯æ°¸è¿œæŒ‚èµ·ï¼ˆå›è°ƒä¸è§¦å‘ï¼‰ã€‚
try:
    from xtquant import xtbson as _BSON_
except ImportError:
    import bson as _BSON_

# è´¢åŠ¡æ•°æ®ä»ç”¨ ThreadPoolExecutorï¼Œfuture.result(timeout=N) åœ¨ Windows ä¸Š
# ä¼šé•¿æ—¶é—´é˜»å¡ä¸»çº¿ç¨‹å¯¼è‡´ Ctrl+C æ— å“åº”ï¼Œæ”¹ç”¨çŸ­è½®è¯¢è®© Python æœ‰æœºä¼šå¤„ç†ä¸­æ–­ã€‚
POLL_INTERVAL = 0.5

try:
    from tqdm import tqdm
except ImportError:
    print("é”™è¯¯: éœ€è¦ tqdm ä¾èµ–ï¼Œè¯·å…ˆæ‰§è¡Œ: pip install tqdm>=4.60")
    print("  æˆ–: pip install -e \".[scripts]\"")
    sys.exit(1)

# é€åªä¸‹è½½ï¼Œå•åªè‚¡ç¥¨çš„å›ºå®šè¶…æ—¶ï¼ˆç§’ï¼‰ã€‚
# æ­£å¸¸ <1 ç§’å®Œæˆï¼Œå¡æ­»çš„ç­‰å†ä¹…ä¹Ÿä¸ä¼šå¥½ï¼Œå¿«é€Ÿè·³è¿‡ã€‚
STOCK_TIMEOUT: dict[str, int] = {
    "1m": 10,
    "5m": 10,
    "15m": 10,
    "30m": 5,
    "60m": 5,
    "1d": 5,
}

# â”€â”€ é»˜è®¤ä¸‹è½½æ¿å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# xtdata.get_sector_list() è¿”å›çš„å¸‚åœºæ¿å—ï¼ˆå…± 30 ä¸ªï¼Œä¸å« SW/CSRC è¡Œä¸šåˆ†ç±»ï¼‰:
#
# è‚¡ç¥¨:
#   æ²ªæ·±Aè‚¡ (5189)    = ä¸Šè¯Aè‚¡ (2306) + æ·±è¯Aè‚¡ (2883)ï¼Œå«åˆ›ä¸šæ¿ (1392) å’Œç§‘åˆ›æ¿ (602)
#   æ²ªæ·±äº¬Aè‚¡ (5501)  = æ²ªæ·±Aè‚¡ + äº¬å¸‚Aè‚¡ (312)
#   æ²ªæ·±Bè‚¡ (79)      = ä¸Šè¯Bè‚¡ (41) + æ·±è¯Bè‚¡ (38)
#   ç§‘åˆ›æ¿CDR (1)
#
# ETF:
#   æ²ªæ·±ETF (1460)    = æ²ªå¸‚ETF (855) + æ·±å¸‚ETF (605)
#
# æŒ‡æ•°:
#   æ²ªæ·±æŒ‡æ•° (609)    = æ²ªå¸‚æŒ‡æ•° (221) + æ·±å¸‚æŒ‡æ•° (388)
#
# è½¬å€º:
#   æ²ªæ·±è½¬å€º (383)    = ä¸Šè¯è½¬å€º (187) + æ·±è¯è½¬å€º (196)
#
# å€ºåˆ¸:
#   æ²ªæ·±å€ºåˆ¸ (39583)  = æ²ªå¸‚å€ºåˆ¸ (21268) + æ·±å¸‚å€ºåˆ¸ (18315)
#
# åŸºé‡‘:
#   æ²ªæ·±åŸºé‡‘ (2004)   = æ²ªå¸‚åŸºé‡‘ (1027) + æ·±å¸‚åŸºé‡‘ (977)
#
# æœŸæƒ:
#   ä¸Šè¯æœŸæƒ (760), æ·±è¯æœŸæƒ (616)
#
# æ¸¯è‚¡:
#   é¦™æ¸¯è”äº¤æ‰€è‚¡ç¥¨ (882), é¦™æ¸¯è”äº¤æ‰€æŒ‡æ•° (0)
#
DEFAULT_SECTORS = "æ²ªæ·±Aè‚¡,æ²ªæ·±ETF,æ²ªæ·±æŒ‡æ•°"

# â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROBE_BATCH_SIZE = 200
SAFETY_OVERLAP_DAYS = 1

# Ctrl+C ä¸­æ–­æ ‡è®°ï¼šxtdata ä¸‹è½½çº¿ç¨‹æ˜¯é daemon çº¿ç¨‹ï¼Œ
# å³ä½¿ executor.shutdown(wait=False) ä¹Ÿæ— æ³•ç»ˆæ­¢å·²è¿è¡Œçš„çº¿ç¨‹ï¼Œ
# Python é€€å‡ºæ—¶ä¼šç­‰å¾…è¿™äº›çº¿ç¨‹å®Œæˆå¯¼è‡´å¡æ­»ã€‚
# è®¾ç½®æ­¤æ ‡è®°å main() ç»“æŸæ—¶ç”¨ os._exit(0) å¼ºåˆ¶é€€å‡ºã€‚
_interrupted = False

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOG_DIR = Path(__file__).resolve().parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logger = logging.getLogger("download_all")
logger.setLevel(logging.DEBUG)

# æ–‡ä»¶ handlerï¼šè¯¦ç»†æ—¥å¿—å†™å…¥ logs/download_all_<date>.log
_log_file = LOG_DIR / f"download_all_{datetime.now():%Y%m%d_%H%M%S}.log"
_fh = logging.FileHandler(_log_file, encoding="utf-8")
_fh.setLevel(logging.DEBUG)
_fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)-7s %(message)s"))
logger.addHandler(_fh)

# æ§åˆ¶å° handlerï¼šä»… WARNING ä»¥ä¸Šï¼ˆé¿å…ä¸ tqdm å†²çªï¼‰
_ch = logging.StreamHandler()
_ch.setLevel(logging.WARNING)
_ch.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
logger.addHandler(_ch)

# â”€â”€ çŠ¶æ€æŒä¹…åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STATE_FILE = LOG_DIR / "download_state.json"
STATE_VERSION = 1


@dataclass
class TaskState:
    """å•ä¸ªä¸‹è½½ä»»åŠ¡çš„çŠ¶æ€ã€‚"""
    last_success_date: str = ""
    last_run_iso: str = ""
    stock_count: int = 0
    ok: int = 0
    fail: int = 0


@dataclass
class DownloadState:
    """å…¨å±€ä¸‹è½½çŠ¶æ€å®¹å™¨ã€‚"""
    version: int = STATE_VERSION
    tasks: dict[str, TaskState] = field(default_factory=dict)


def load_state() -> DownloadState:
    """è¯»å–çŠ¶æ€æ–‡ä»¶ï¼Œå¼‚å¸¸æ—¶å›é€€ç©ºçŠ¶æ€ã€‚"""
    if not STATE_FILE.exists():
        return DownloadState()
    try:
        raw = json.loads(STATE_FILE.read_text(encoding="utf-8"))
        state = DownloadState(version=raw.get("version", STATE_VERSION))
        for key, val in raw.get("tasks", {}).items():
            state.tasks[key] = TaskState(**val)
        return state
    except Exception as exc:
        logger.warning("è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨ç©ºçŠ¶æ€: %s", exc)
        return DownloadState()


def save_state(state: DownloadState) -> None:
    """å°†çŠ¶æ€å†™å…¥ JSON æ–‡ä»¶ã€‚"""
    data = {
        "version": state.version,
        "tasks": {k: asdict(v) for k, v in state.tasks.items()},
    }
    STATE_FILE.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
    logger.info("çŠ¶æ€å·²ä¿å­˜: %s", STATE_FILE)


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_batches(lst: list, size: int) -> list[list]:
    """å°†åˆ—è¡¨æŒ‰ size åˆ‡åˆ†ä¸ºå­åˆ—è¡¨ã€‚"""
    return [lst[i : i + size] for i in range(0, len(lst), size)]


def _download_single_kline(
    client,
    code: str,
    period: str,
    start_time: str,
    end_time: str,
    incrementally: bool | None,
    timeout: float,
) -> str:
    """ç›´æ¥è°ƒç”¨ client.supply_history_data2() ä¸‹è½½å•åªè‚¡ç¥¨ K çº¿ã€‚

    ç»•è¿‡ xtquant.download_history_data2 çš„ bugï¼š
    å½“ result=Trueï¼ˆæ•°æ®å·²ç¼“å­˜ï¼‰æ—¶ï¼Œxtquant çš„è½®è¯¢å¾ªç¯ä¼šæ°¸è¿œæŒ‚èµ·ï¼Œ
    å› ä¸ºå›è°ƒæ°¸è¿œä¸ä¼šè¢«è§¦å‘ã€‚

    Args:
        incrementally: True=å¢é‡, False=å…¨é‡, None=è‡ªåŠ¨(start_time éç©ºâ†’False, ç©ºâ†’True)ã€‚

    Returns:
        "ok" | "timeout" | "cached" | "error: ..." | "disconnected"
    """
    # è§£æ incrementally: None â†’ æ ¹æ® start_time è‡ªåŠ¨å†³å®š
    if incrementally is None:
        incrementally = not bool(start_time)
    param = {"incrementally": incrementally}
    bson_param = _BSON_.BSON.encode(param)

    status = {"done": False, "error": ""}

    def on_progress(data):
        total_val = data.get("total", 0)
        if total_val < 0:
            status["error"] = data.get("message", "unknown error")
            status["done"] = True
            return True
        finished = data.get("finished", 0)
        if finished >= total_val and total_val > 0:
            status["done"] = True
        return status["done"]

    result = client.supply_history_data2(
        [code], period, start_time, end_time, bson_param, on_progress,
    )

    if result:
        # result=True: æ•°æ®å·²ç¼“å­˜ï¼ŒC++ åŒæ­¥è¿”å›ï¼Œå›è°ƒä¸ä¼šè§¦å‘ã€‚
        # è¿™å°±æ˜¯ xtquant download_history_data2 çš„ bug æ‰€åœ¨:
        # å®ƒåœ¨æ­¤æƒ…å†µä¸‹ä»ç„¶æ— é™è½®è¯¢ç­‰å¾…å›è°ƒï¼Œå¯¼è‡´æ°¸è¿œæŒ‚èµ·ã€‚
        return "cached"

    # result=False: å¼‚æ­¥ä¸‹è½½ï¼Œè½®è¯¢ç­‰å¾…å®Œæˆ
    deadline = time.monotonic() + timeout
    while not status["done"]:
        if not client.is_connected():
            return "disconnected"
        if time.monotonic() >= deadline:
            return "timeout"
        time.sleep(0.1)  # 0.1s è½®è¯¢ï¼ŒKeyboardInterrupt å¯åœ¨æ­¤å¤„è¢«æ•è·

    if status["error"]:
        return f"error: {status['error']}"
    return "ok"


def _wait_future(future, timeout: float) -> None:
    """ç­‰å¾… future å®Œæˆï¼Œæ¯ POLL_INTERVAL ç§’é†’æ¥æ£€æŸ¥ KeyboardInterruptã€‚

    åœ¨ Windows ä¸Š future.result(timeout=N) ä¼šé•¿æ—¶é—´é˜»å¡ä¸»çº¿ç¨‹ï¼Œ
    Ctrl+C ä¿¡å·è¦ç­‰åˆ° timeout åˆ°æœŸæ‰èƒ½è¢«å¤„ç†ã€‚
    è¿™é‡Œç”¨çŸ­è½®è¯¢æ›¿ä»£ï¼Œç¡®ä¿ Ctrl+C èƒ½åœ¨ 0.5 ç§’å†…å“åº”ã€‚
    """
    deadline = time.monotonic() + timeout
    while True:
        remaining = deadline - time.monotonic()
        if remaining <= 0:
            raise FutureTimeoutError()
        try:
            future.result(timeout=min(remaining, POLL_INTERVAL))
            return  # æˆåŠŸå®Œæˆ
        except FutureTimeoutError:
            if time.monotonic() >= deadline:
                raise
            # æœªè¶…æ—¶ï¼Œç»§ç»­è½®è¯¢ï¼ˆæ­¤å¤„ KeyboardInterrupt å¯è¢«æ•è·ï¼‰


def _run_kline_downloads(
    client,
    stocks: list[str],
    stock_indices: list[int],
    period: str,
    start_time: str,
    end_time: str,
    incrementally: bool | None,
    timeout: int,
    pbar: tqdm,
    label: str,
) -> tuple[int, int, int, list[int], bool]:
    """é€åªä¸‹è½½ K çº¿æ•°æ®ï¼ˆç›´æ¥è°ƒç”¨ client.supply_history_data2ï¼‰ã€‚

    Args:
        client: xtdata.get_client() è¿”å›çš„ C++ å®¢æˆ·ç«¯å¯¹è±¡ã€‚
        incrementally: True=å¢é‡, False=å…¨é‡, None=è‡ªåŠ¨å†³å®šã€‚

    Returns:
        (ok_count, fail_count, timeout_count, failed_indices, interrupted)
    """
    ok_count = 0
    fail_count = 0
    timeout_count = 0
    failed_indices: list[int] = []
    n_total = len(stock_indices)

    for seq, idx in enumerate(stock_indices):
        code = stocks[idx]
        pbar.set_description(f"{label} [{seq+1}/{n_total}]")
        parts = [code]
        if fail_count or timeout_count:
            parts.append(f"å¤±è´¥:{fail_count} è¶…æ—¶:{timeout_count}")
        pbar.set_postfix_str(" | ".join(parts), refresh=True)

        try:
            result = _download_single_kline(
                client, code, period, start_time, end_time,
                incrementally, timeout,
            )
            if result in ("ok", "cached"):
                ok_count += 1
                logger.debug("Kçº¿ %s %s %s", period, code, result)
            elif result == "timeout":
                timeout_count += 1
                fail_count += 1
                failed_indices.append(idx)
                logger.error("Kçº¿ %s %s è¶…æ—¶ (%dç§’)", period, code, timeout)
                tqdm.write(f"  âš  {code} è¶…æ—¶ ({timeout}s)")
            elif result == "disconnected":
                fail_count += 1
                failed_indices.append(idx)
                logger.error("Kçº¿ %s %s è¿æ¥æ–­å¼€", period, code)
                tqdm.write(f"  âš  {code} è¿æ¥æ–­å¼€")
            else:
                # "error: ..." æ¶ˆæ¯
                fail_count += 1
                failed_indices.append(idx)
                logger.error("Kçº¿ %s %s %s", period, code, result)
                tqdm.write(f"  âš  {code} {result}")
        except KeyboardInterrupt:
            global _interrupted
            _interrupted = True
            logger.warning("Kçº¿ %s è¢«ç”¨æˆ·ä¸­æ–­", period)
            tqdm.write(f"\n  ç”¨æˆ·ä¸­æ–­ï¼ŒKçº¿ {period} æœ¬è½®å·²å®Œæˆ {ok_count} åª")
            return ok_count, fail_count, timeout_count, failed_indices, True
        except Exception as exc:
            fail_count += 1
            failed_indices.append(idx)
            logger.error("Kçº¿ %s %s å¼‚å¸¸: %s", period, code, exc)
            tqdm.write(f"  âš  {code} å¼‚å¸¸: {exc}")
        finally:
            pbar.update(1)

    return ok_count, fail_count, timeout_count, failed_indices, False


def _make_financial_cb(
    flag: list[bool], codes: list[str], tables: list[str],
    fail_count: int, timeout_count: int, pbar: tqdm,
) -> callable:
    """åˆ›å»ºè´¢åŠ¡æ•°æ®ä¸‹è½½å›è°ƒï¼Œç”¨äºæ›´æ–° tqdm è¿›åº¦æ¡ã€‚"""
    n_codes = len(codes)
    n_tables = len(tables)
    def _on_progress(data: dict) -> None:
        if flag[0]:
            return
        finished = data.get("finished", 0)
        total = data.get("total", 0)
        parts = [f"æ‰¹å†… {finished}/{total}"]
        if total > 0:
            item_est = min(int(finished * n_codes * n_tables / total), n_codes * n_tables) - 1
            if item_est >= 0:
                stock_idx = item_est // n_tables
                table_idx = item_est % n_tables
                if stock_idx < n_codes:
                    parts.append(f"{codes[stock_idx]}/{tables[table_idx]}")
        if fail_count or timeout_count:
            parts.append(f"å¤±è´¥:{fail_count} è¶…æ—¶:{timeout_count}")
        pbar.set_postfix_str(" | ".join(parts), refresh=True)
    return _on_progress


def _run_financial_batches(
    batches: list[list[str]],
    batch_indices: list[int],
    table_list: list[str],
    timeout: int,
    delay: float,
    pbar: tqdm,
    label: str,
) -> tuple[int, int, int, list[int], bool]:
    """æ‰§è¡Œä¸€è½®è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ä¸‹è½½ã€‚

    Returns:
        (ok_count, fail_count, timeout_count, failed_indices, interrupted)
    """
    ok_count = 0
    fail_count = 0
    timeout_count = 0
    failed_indices: list[int] = []
    n_total = len(batch_indices)

    for seq, idx in enumerate(batch_indices):
        batch = batches[idx]
        batch_items = len(batch) * len(table_list)
        cancelled = [False]
        pbar.set_description(f"{label} [{seq+1}/{n_total}æ‰¹]")

        executor = ThreadPoolExecutor(max_workers=1)
        try:
            future = executor.submit(
                xtdata.download_financial_data2,
                stock_list=batch,
                table_list=table_list,
                callback=_make_financial_cb(cancelled, batch, table_list, fail_count, timeout_count, pbar),
            )
            _wait_future(future, timeout)
            ok_count += len(batch)
            logger.debug("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d æˆåŠŸ (%d åª)", idx+1, len(batch))
        except FutureTimeoutError:
            cancelled[0] = True
            timeout_count += 1
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d è¶…æ—¶ (%dç§’, %d åª)", idx+1, timeout, len(batch))
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} è¶…æ—¶ ({timeout}s, {len(batch)} åª)")
        except KeyboardInterrupt:
            global _interrupted  # noqa: PLW0602 (already declared in kline handler)
            _interrupted = True
            cancelled[0] = True
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.close()
            logger.warning("è´¢åŠ¡æ•°æ®è¢«ç”¨æˆ·ä¸­æ–­")
            tqdm.write(f"\n  ç”¨æˆ·ä¸­æ–­ï¼Œè´¢åŠ¡æ•°æ®æœ¬è½®å·²å®Œæˆ {ok_count} åª")
            return ok_count, fail_count, timeout_count, failed_indices, True
        except Exception as exc:
            cancelled[0] = True
            fail_count += len(batch)
            failed_indices.append(idx)
            logger.error("è´¢åŠ¡æ•°æ®æ‰¹æ¬¡ %d å¤±è´¥ (%d åª): %s", idx+1, len(batch), exc)
            tqdm.write(f"  âš  æ‰¹æ¬¡ {idx+1} å¤±è´¥ ({len(batch)} åª): {exc}")
        finally:
            executor.shutdown(wait=False, cancel_futures=True)
            pbar.update(batch_items)

        if delay > 0 and seq < n_total - 1:
            time.sleep(delay)
    else:
        pbar.close()

    return ok_count, fail_count, timeout_count, failed_indices, False


# è´¢åŠ¡æ•°æ®çš„å…¬å‘Šæ—¥æœŸè·ä»Šè¶…è¿‡æ­¤å¤©æ•°ï¼Œè®¤ä¸ºæ•°æ®å¯èƒ½è¿‡æœŸï¼Œéœ€é‡æ–°ä¸‹è½½ã€‚
# ä¸­å›½ä¸Šå¸‚å…¬å¸å­£æŠ¥æŠ«éœ²å‘¨æœŸ: Q1(4/30å‰), H1(8/31å‰), Q3(10/31å‰), å¹´æŠ¥(4/30å‰)ã€‚
# 90 å¤©åŸºæœ¬è¦†ç›–ä¸€ä¸ªå­£åº¦é—´éš”ã€‚
FINANCIAL_STALE_DAYS = 90


def probe_financial_cache(
    stocks: list[str], table_list: list[str],
) -> tuple[set[str], int]:
    """æ¢æµ‹å“ªäº›è‚¡ç¥¨å·²æœ‰æ–°é²œçš„æœ¬åœ°è´¢åŠ¡æ•°æ®ç¼“å­˜ã€‚

    åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæŠ¥è¡¨çš„æœ€æ–°å…¬å‘Šæ—¥æœŸ(m_anntime)ï¼š
    - è·ä»Š â‰¤ FINANCIAL_STALE_DAYS â†’ æ–°é²œï¼Œè·³è¿‡
    - è·ä»Š > FINANCIAL_STALE_DAYS â†’ è¿‡æœŸï¼Œéœ€é‡æ–°ä¸‹è½½

    Returns:
        (æ–°é²œçš„è‚¡ç¥¨ä»£ç é›†åˆ, è¿‡æœŸè‚¡ç¥¨æ•°é‡)
    """
    fresh: set[str] = set()
    stale_count = 0
    check_table = table_list[0]
    stale_cutoff = (datetime.now() - timedelta(days=FINANCIAL_STALE_DAYS)).strftime("%Y%m%d")

    probe_pbar = tqdm(total=len(stocks), desc="æ¢æµ‹è´¢åŠ¡ç¼“å­˜", unit="åª")
    for batch in make_batches(stocks, PROBE_BATCH_SIZE):
        try:
            data = xtdata.get_financial_data(batch, [check_table])
            for stock, tables_data in data.items():
                if not isinstance(tables_data, dict):
                    continue
                df = tables_data.get(check_table)
                if df is None or not isinstance(df, pd.DataFrame) or df.empty:
                    continue
                # æ£€æŸ¥æœ€æ–°å…¬å‘Šæ—¥æœŸ
                if "m_anntime" in df.columns:
                    max_ann = df["m_anntime"].dropna()
                    if not max_ann.empty:
                        latest = str(max_ann.max())
                        if latest >= stale_cutoff:
                            fresh.add(stock)
                        else:
                            stale_count += 1
                    else:
                        stale_count += 1
                else:
                    # æ— æ³•åˆ¤æ–­æ—¥æœŸï¼Œä¿å®ˆè®¤ä¸ºæ–°é²œ
                    fresh.add(stock)
        except Exception as exc:
            logger.warning("è´¢åŠ¡ç¼“å­˜æ¢æµ‹æ‰¹æ¬¡å¤±è´¥: %s", exc)
        probe_pbar.update(len(batch))
    probe_pbar.close()
    return fresh, stale_count


def download_financial(
    stocks: list[str],
    table_list: list[str],
    batch_size: int,
    timeout: int = 120,
    delay: float = 0.2,
    max_retries: int = 2,
    full: bool = False,
) -> dict[str, int]:
    """ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼Œè¿”å› {"ok": n, "fail": n, "timeout": n}ã€‚

    é€šè¿‡ callback å®ç°é€é¡¹ï¼ˆè‚¡ç¥¨ Ã— æŠ¥è¡¨ï¼‰ç²’åº¦çš„è¿›åº¦æ›´æ–°ã€‚
    æ¯æ‰¹ä¸‹è½½æœ‰è¶…æ—¶ä¿æŠ¤ï¼Œæ‰¹æ¬¡é—´æœ‰å»¶è¿Ÿä»¥ç¼“è§£æœåŠ¡ç«¯å‹åŠ›ã€‚
    è¶…æ—¶å¤±è´¥çš„æ‰¹æ¬¡ä¼šè‡ªåŠ¨é‡è¯•ï¼Œæ¯è½®é‡è¯•è¶…æ—¶å†å¢åŠ  50%ã€‚

    full=False æ—¶å…ˆæ¢æµ‹ç¼“å­˜ï¼Œè·³è¿‡å·²æœ‰æ•°æ®çš„è‚¡ç¥¨ã€‚
    """
    # â”€â”€ ç¼“å­˜æ¢æµ‹ â”€â”€
    n_original = len(stocks)
    if not full:
        tqdm.write("æ¢æµ‹è´¢åŠ¡æ•°æ®æœ¬åœ°ç¼“å­˜...")
        fresh, n_stale = probe_financial_cache(stocks, table_list)
        need_download = [s for s in stocks if s not in fresh]
        n_fresh = len(fresh)
        n_no_data = len(need_download) - n_stale
        if n_fresh:
            tqdm.write(f"  Â· {n_fresh} åªç¼“å­˜æ–°é²œ (â‰¤{FINANCIAL_STALE_DAYS}å¤©)ï¼Œè·³è¿‡")
        if n_stale:
            tqdm.write(f"  Â· {n_stale} åªç¼“å­˜è¿‡æœŸ (>{FINANCIAL_STALE_DAYS}å¤©)ï¼Œé‡æ–°ä¸‹è½½")
        if n_no_data:
            tqdm.write(f"  Â· {n_no_data} åªæ— ç¼“å­˜ï¼Œå…¨æ–°ä¸‹è½½")
        logger.info(
            "è´¢åŠ¡ç¼“å­˜æ¢æµ‹: æ–°é²œ %d, è¿‡æœŸ %d, æ— ç¼“å­˜ %d",
            n_fresh, n_stale, n_no_data,
        )
        if not need_download:
            tqdm.write("  Â· å…¨éƒ¨å·²æœ‰ç¼“å­˜ï¼Œè·³è¿‡è´¢åŠ¡æ•°æ®ä¸‹è½½")
            return {"ok": n_original, "fail": 0, "timeout": 0}
        stocks = need_download

    batches = make_batches(stocks, batch_size)
    total_items = len(stocks) * len(table_list)
    n_batches = len(batches)
    all_indices = list(range(n_batches))

    logger.info(
        "å¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼Œå…± %d æ‰¹ (%d åª Ã— %d è¡¨ = %d é¡¹)",
        n_batches, len(stocks), len(table_list), total_items,
    )
    pbar = tqdm(total=total_items, desc="è´¢åŠ¡", unit="é¡¹")

    # â”€â”€ é¦–è½®ä¸‹è½½ â”€â”€
    ok, fail, to, failed, interrupted = _run_financial_batches(
        batches, all_indices, table_list, timeout, delay, pbar, "è´¢åŠ¡",
    )

    # â”€â”€ è‡ªåŠ¨é‡è¯•å¤±è´¥æ‰¹æ¬¡ â”€â”€
    for retry_round in range(1, max_retries + 1):
        if not failed or interrupted:
            break
        retry_timeout = int(timeout * (1.5 ** retry_round))
        n_retry = len(failed)
        retry_stocks = sum(len(batches[i]) for i in failed)
        retry_items = retry_stocks * len(table_list)
        tqdm.write(
            f"  ğŸ”„ è´¢åŠ¡æ•°æ®é‡è¯•ç¬¬ {retry_round}/{max_retries} è½®: "
            f"{n_retry} ä¸ªæ‰¹æ¬¡ ({retry_stocks} åª), è¶…æ—¶ {retry_timeout}s"
        )
        logger.info(
            "è´¢åŠ¡æ•°æ®é‡è¯•ç¬¬ %d è½®: %d ä¸ªæ‰¹æ¬¡ (%d åª), è¶…æ—¶ %ds",
            retry_round, n_retry, retry_stocks, retry_timeout,
        )
        retry_pbar = tqdm(total=retry_items, desc=f"è´¢åŠ¡ é‡è¯•{retry_round}", unit="é¡¹")
        r_ok, r_fail, r_to, still_failed, interrupted = _run_financial_batches(
            batches, failed, table_list, retry_timeout, delay, retry_pbar, f"è´¢åŠ¡ é‡è¯•{retry_round}",
        )
        ok += r_ok
        failed = still_failed

    # æœ€ç»ˆä¿®æ­£è®¡æ•°ï¼ˆå«ç¼“å­˜è·³è¿‡çš„è‚¡ç¥¨ï¼‰
    n_cached = n_original - len(stocks)
    final_fail_stocks = sum(len(batches[i]) for i in failed)
    ok = len(stocks) - final_fail_stocks if not interrupted else ok
    ok += n_cached  # ç¼“å­˜è·³è¿‡çš„ä¹Ÿè®¡å…¥æˆåŠŸ
    fail = final_fail_stocks
    to = len(failed)

    logger.info("è´¢åŠ¡æ•°æ®å®Œæˆ: æˆåŠŸ %d (ç¼“å­˜ %d), å¤±è´¥ %d (è¶…æ—¶ %d)", ok, n_cached, fail, to)
    if failed:
        logger.warning("è´¢åŠ¡æ•°æ®æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: %s", failed)
        tqdm.write(f"  è´¢åŠ¡æ•°æ®æœ€ç»ˆå¤±è´¥æ‰¹æ¬¡ç´¢å¼•: {failed}")

    return {"ok": ok, "fail": fail, "timeout": to}


# â”€â”€ v2 æ–°å¢ï¼šç¼“å­˜æ¢æµ‹ä¸åˆ†ç»„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def probe_local_dates(stocks: list[str], period: str) -> dict[str, str]:
    """æ‰¹é‡æ¢æµ‹æ¯åªè‚¡ç¥¨æœ¬åœ°ç¼“å­˜çš„æœ€æ–°æ•°æ®æ—¥æœŸã€‚

    å¯¹å…¨éƒ¨è‚¡ç¥¨åˆ†æ‰¹è°ƒç”¨ get_local_data(count=1)ï¼Œ
    æ¯æ‰¹ 200 åªï¼Œæ¯åªä»…è¿”å›æœ€å 1 æ¡è®°å½•ã€‚

    Returns:
        {stock_code: "YYYYMMDD"} â€” æ— æœ¬åœ°æ•°æ®çš„è‚¡ç¥¨ä¸åœ¨å­—å…¸ä¸­ã€‚
    """
    result: dict[str, str] = {}
    probe_pbar = tqdm(total=len(stocks), desc="æ¢æµ‹æœ¬åœ°ç¼“å­˜", unit="åª")
    for i in range(0, len(stocks), PROBE_BATCH_SIZE):
        batch = stocks[i : i + PROBE_BATCH_SIZE]
        try:
            data = xtdata.get_local_data(
                field_list=[], stock_list=batch,
                period=period, start_time="", end_time="", count=1,
            )
            for stock, df in data.items():
                if df is not None and not df.empty:
                    last_ts = df.index[-1]
                    if isinstance(last_ts, (int, float)):
                        dt = datetime.fromtimestamp(last_ts / 1000)
                    else:
                        dt = pd.Timestamp(last_ts).to_pydatetime()
                    result[stock] = dt.strftime("%Y%m%d")
        except Exception as exc:
            logger.warning("ç¼“å­˜æ¢æµ‹æ‰¹æ¬¡å¤±è´¥: %s", exc)
        probe_pbar.update(len(batch))
    probe_pbar.close()
    return result


def group_stocks_by_date(
    stocks: list[str],
    local_dates: dict[str, str],
) -> list[tuple[str, list[str]]]:
    """æŒ‰æœ¬åœ°ç¼“å­˜æœ€æ–°æ—¥æœŸåˆ†ç»„ã€‚

    æœ‰ç¼“å­˜çš„è‚¡ç¥¨: start_time = last_date - SAFETY_OVERLAP_DAYS
    æ— ç¼“å­˜çš„è‚¡ç¥¨: start_time = "" (å…¨é‡)

    Returns:
        [(start_time, [stock_codes]), ...] æŒ‰ start_time æ’åºï¼ˆ""åœ¨æœ€å‰ï¼‰ã€‚
    """
    groups: dict[str, list[str]] = defaultdict(list)
    for stock in stocks:
        last_date = local_dates.get(stock)
        if last_date:
            overlap_dt = datetime.strptime(last_date, "%Y%m%d") - timedelta(days=SAFETY_OVERLAP_DAYS)
            groups[overlap_dt.strftime("%Y%m%d")].append(stock)
        else:
            groups[""].append(stock)
    return sorted(groups.items(), key=lambda x: x[0])


# â”€â”€ å¹´åº¦åˆ†æ®µä¸‹è½½æ”¯æŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def probe_year_coverage(
    stocks: list[str], period: str, since_year: int,
) -> dict[int, set[str]]:
    """æ¢æµ‹æ¯ä¸ªå¹´ä»½ä¸­å“ªäº›è‚¡ç¥¨å·²æœ‰æœ¬åœ°ç¼“å­˜ã€‚

    å¯¹ [since_year, current_year] èŒƒå›´å†…çš„æ¯ä¸ªå¹´ä»½ï¼Œ
    åˆ†æ‰¹è°ƒç”¨ get_local_data(count=1) æ£€æµ‹æ˜¯å¦æœ‰æ•°æ®ã€‚

    Returns:
        {year: {æœ‰ç¼“å­˜æ•°æ®çš„ stock_code é›†åˆ}}
    """
    current_year = datetime.now().year
    years = list(range(since_year, current_year + 1))
    coverage: dict[int, set[str]] = {y: set() for y in years}
    total_probes = len(stocks) * len(years)
    probe_pbar = tqdm(total=total_probes, desc="æ¢æµ‹å¹´åº¦ç¼“å­˜", unit="åª")
    for year in years:
        for batch in make_batches(stocks, PROBE_BATCH_SIZE):
            try:
                data = xtdata.get_local_data(
                    field_list=[], stock_list=batch, period=period,
                    start_time=f"{year}0101", end_time=f"{year}1231", count=1,
                )
                for stock, df in data.items():
                    if df is not None and not df.empty:
                        coverage[year].add(stock)
            except Exception as exc:
                logger.warning("å¹´åº¦ç¼“å­˜æ¢æµ‹å¤±è´¥ (year=%d): %s", year, exc)
            probe_pbar.update(len(batch))
    probe_pbar.close()
    return coverage


def build_year_groups(
    stocks: list[str],
    period: str,
    since_year: int,
    full: bool,
) -> list[tuple[str, str, list[str]]]:
    """æ„å»ºå¹´åº¦ä¸‹è½½ä»»åŠ¡åˆ—è¡¨ï¼Œä»æœ€è¿‘å¹´ä»½åˆ°æœ€è¿œå¹´ä»½æ’åˆ—ã€‚

    full=True æ—¶è·³è¿‡æ¢æµ‹ï¼Œè§†ä¸ºå…¨éƒ¨æ— ç¼“å­˜ã€‚

    Returns:
        [(start_time, end_time, [stocks]), ...]
        å½“å‰å¹´ end_time=""ï¼Œå†å²å¹´ end_time="YYYY1231"ã€‚
    """
    current_year = datetime.now().year
    years = list(range(since_year, current_year + 1))

    if full:
        # --full æ¨¡å¼: ä¸æ¢æµ‹ï¼Œå…¨éƒ¨è§†ä¸ºæ— ç¼“å­˜
        coverage: dict[int, set[str]] = {y: set() for y in years}
    else:
        tqdm.write(f"\næ¢æµ‹ {period} å¹´åº¦ç¼“å­˜ ({since_year}-{current_year})...")
        coverage = probe_year_coverage(stocks, period, since_year)

    # æŒ‰æ¯åªè‚¡ç¥¨çš„å·²ç¼“å­˜å¹´ä»½æ•°æ’åºï¼šç¼“å­˜æœ€å°‘ï¼ˆç¼ºå£æœ€å¤§ï¼‰çš„ä¼˜å…ˆä¸‹è½½
    stock_cached_years = {
        s: sum(1 for y in years if s in coverage.get(y, set()))
        for s in stocks
    }

    groups: list[tuple[str, str, list[str]]] = []
    # ä»æœ€è¿‘å¹´ä»½åˆ°æœ€è¿œå¹´ä»½
    for year in reversed(years):
        cached = coverage.get(year, set())
        need_download = sorted(
            [s for s in stocks if s not in cached],
            key=lambda s: stock_cached_years[s],
        )
        if year == current_year:
            end_time = ""  # å½“å‰å¹´è·å–æœ€æ–°æ•°æ®
            range_label = f"{year}0101 ~ è‡³ä»Š"
        else:
            end_time = f"{year}1231"
            range_label = f"{year}0101 ~ {year}1231"
        if not need_download:
            tqdm.write(f"  Â· {year}: å…¨éƒ¨ {len(stocks)} åªå·²æœ‰ç¼“å­˜ï¼Œè·³è¿‡")
            logger.info("å¹´åº¦ %d: å…¨éƒ¨ %d åªå·²æœ‰ç¼“å­˜ï¼Œè·³è¿‡", year, len(stocks))
        else:
            skipped = len(stocks) - len(need_download)
            tqdm.write(
                f"  Â· {year} ({range_label}): "
                f"éœ€ä¸‹è½½ {len(need_download)} åª, è·³è¿‡ {skipped} åª"
            )
            logger.info(
                "å¹´åº¦ %d (%s): éœ€ä¸‹è½½ %d åª, è·³è¿‡ %d åª",
                year, range_label, len(need_download), skipped,
            )
            groups.append((f"{year}0101", end_time, need_download))

    return groups


# â”€â”€ v2 æ–°å¢ï¼šåˆ†ç»„ä¸‹è½½ä¸»å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_kline_v2(
    stocks: list[str],
    periods: list[str],
    full: bool,
    max_retries: int,
    since_year: int | None = None,
) -> dict[str, dict[str, int]]:
    """é€åªä¸‹è½½ K çº¿æ•°æ®ã€‚

    ä¸‰ç§æ¨¡å¼:
    A. --since YYYY: æŒ‰å¹´åº¦åˆ†æ®µä¸‹è½½ï¼Œè‡ªåŠ¨è·³è¿‡å·²ç¼“å­˜å¹´ä»½
    B. --full (æ—  --since): æ‰€æœ‰è‚¡ç¥¨ç»Ÿä¸€ start_time=""
    C. é»˜è®¤: é€è‚¡ç²¾å‡†å¢é‡ï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„ä¸‹è½½

    Returns:
        {period: {"ok": n, "fail": n, "timeout": n, "date_groups": n}}
    """
    results: dict[str, dict[str, int]] = {}
    client = xtdata.get_client()

    for period in periods:
        effective_timeout = STOCK_TIMEOUT.get(period, 10)
        tqdm.write(f"  å‘¨æœŸ {period} å•åªè¶…æ—¶: {effective_timeout}s")
        logger.info("Kçº¿ %s å•åªè¶…æ—¶: %ds", period, effective_timeout)

        # å¹´åº¦æ¨¡å¼ä¸é‡è¯•ï¼šå¤±è´¥çš„è‚¡ç¥¨é‡è·‘å‘½ä»¤ä¼šè‡ªåŠ¨è·³è¿‡å·²ç¼“å­˜æ•°æ®
        effective_retries = 0 if since_year is not None else max_retries

        if since_year is not None:
            # æ¨¡å¼ A: å¹´åº¦åˆ†æ®µä¸‹è½½ (--since)
            # å¤–å±‚å·²é€šè¿‡ probe_year_coverage è·³è¿‡å·²ç¼“å­˜å¹´ä»½ï¼Œ
            # æ— éœ€ xtquant å†…éƒ¨å†åšå¢é‡æ‰«æï¼Œç”¨ None è®©å…¶è‡ªåŠ¨å†³å®š
            # (start_time éç©ºæ—¶è‡ªåŠ¨ Falseï¼Œçœå»ç¼“å­˜æ‰«æå¼€é”€)
            date_groups = build_year_groups(stocks, period, since_year, full)
            incrementally = None
        elif full:
            # æ¨¡å¼ B: ä¼ ç»Ÿå…¨é‡ (--full, æ—  --since)
            date_groups = [("", "", stocks)]
            incrementally = None
        else:
            # æ¨¡å¼ C: é€è‚¡ç²¾å‡†å¢é‡ï¼Œä»ä¸Šæ¬¡ç¼“å­˜æ—¥æœŸç»­ä¸‹ï¼Œå¿…é¡»å¢é‡
            incrementally = True
            tqdm.write(f"\næ¢æµ‹ {period} æœ¬åœ°ç¼“å­˜...")
            local_dates = probe_local_dates(stocks, period)
            today_str = datetime.now().strftime("%Y%m%d")
            # æŒ‰ç¼ºå£å¤©æ•°é™åºæ’åˆ—ï¼šæ— ç¼“å­˜ > ç¼“å­˜æœ€æ—§ > ç¼“å­˜æœ€æ–°
            def _gap_sort_key(s: str) -> int:
                d = local_dates.get(s)
                if not d:
                    return 999999  # æ— ç¼“å­˜ï¼Œç¼ºå£æœ€å¤§
                return (datetime.strptime(today_str, "%Y%m%d") - datetime.strptime(d, "%Y%m%d")).days
            sorted_stocks = sorted(stocks, key=_gap_sort_key, reverse=True)
            # æ¯åªè‚¡ç¥¨ç”¨è‡ªå·±ç²¾ç¡®çš„ start_time
            date_groups = []
            for s in sorted_stocks:
                d = local_dates.get(s)
                if d:
                    overlap_dt = datetime.strptime(d, "%Y%m%d") - timedelta(days=SAFETY_OVERLAP_DAYS)
                    st = overlap_dt.strftime("%Y%m%d")
                else:
                    st = ""
                date_groups.append((st, "", [s]))
            # æ‰“å°æ‘˜è¦
            n_no_cache = sum(1 for s in sorted_stocks if s not in local_dates)
            n_cached = len(sorted_stocks) - n_no_cache
            if n_no_cache:
                tqdm.write(f"  Â· {n_no_cache} åªæ— ç¼“å­˜ (å…¨é‡ä¸‹è½½)")
            if n_cached:
                oldest = min(local_dates.values())
                newest = max(local_dates.values())
                tqdm.write(f"  Â· {n_cached} åªæœ‰ç¼“å­˜ (æœ€æ—§ {oldest}, æœ€æ–° {newest})")

        n_date_groups = len(date_groups)

        # æŒ‰ç»„é€åªä¸‹è½½
        total_stocks = sum(len(g) for _, _, g in date_groups)
        pbar = tqdm(total=total_stocks, desc=f"Kçº¿ {period}", unit="åª")
        total_ok = 0
        total_fail = 0
        total_to = 0
        interrupted = False

        for start_time, end_time, group_stocks in date_groups:
            all_indices = list(range(len(group_stocks)))
            st_label = start_time or "(å…¨é‡)"
            et_label = end_time or "(è‡³ä»Š)"
            logger.info(
                "å¼€å§‹ä¸‹è½½ K çº¿ %sï¼Œç»„ start=%s end=%sï¼Œå…± %d åª, è¶…æ—¶ %ds",
                period, st_label, et_label, len(group_stocks), effective_timeout,
            )

            ok, fail, to, failed, interrupted = _run_kline_downloads(
                client, group_stocks, all_indices, period, start_time, end_time,
                incrementally, effective_timeout, pbar, f"Kçº¿ {period}",
            )

            # è‡ªåŠ¨é‡è¯•å¤±è´¥è‚¡ç¥¨
            for retry_round in range(1, effective_retries + 1):
                if not failed or interrupted:
                    break
                retry_timeout = int(effective_timeout * (1.5 ** retry_round))
                n_retry = len(failed)
                tqdm.write(
                    f"  ğŸ”„ Kçº¿ {period} é‡è¯•ç¬¬ {retry_round}/{effective_retries} è½®: "
                    f"{n_retry} åª, è¶…æ—¶ {retry_timeout}s"
                )
                logger.info(
                    "Kçº¿ %s é‡è¯•ç¬¬ %d è½®: %d åª, è¶…æ—¶ %ds",
                    period, retry_round, n_retry, retry_timeout,
                )
                retry_pbar = tqdm(total=n_retry, desc=f"Kçº¿ {period} é‡è¯•{retry_round}", unit="åª")
                r_ok, r_fail, r_to, still_failed, interrupted = _run_kline_downloads(
                    client, group_stocks, failed, period, start_time, end_time,
                    incrementally, retry_timeout, retry_pbar,
                    f"Kçº¿ {period} é‡è¯•{retry_round}",
                )
                retry_pbar.close()
                ok += r_ok
                failed = still_failed

            final_fail = len(failed)
            ok = len(group_stocks) - final_fail if not interrupted else ok
            total_ok += ok
            total_fail += final_fail
            total_to += len(failed)

            if failed:
                failed_codes = [group_stocks[i] for i in failed[:10]]
                suffix = f" ...ç­‰ {len(failed)} åª" if len(failed) > 10 else ""
                logger.warning("Kçº¿ %s (start=%s) æœ€ç»ˆå¤±è´¥: %s%s", period, st_label, failed_codes, suffix)
                tqdm.write(f"  {period} (start={st_label}) å¤±è´¥ {len(failed)} åª")

            if interrupted:
                break

        pbar.close()
        results[period] = {
            "ok": total_ok, "fail": total_fail, "timeout": total_to,
            "date_groups": n_date_groups,
        }
        logger.info(
            "Kçº¿ %s å®Œæˆ: æˆåŠŸ %d, å¤±è´¥ %d (è¶…æ—¶ %d), æ—¥æœŸç»„ %d",
            period, total_ok, total_fail, total_to, n_date_groups,
        )
        if interrupted:
            break

    return results


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="é€è‚¡ç²¾å‡†å¢é‡ä¸‹è½½æ²ªæ·± A è‚¡å†å²è¡Œæƒ… + è´¢åŠ¡æ•°æ® (v2)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        default=False,
        help="å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼ˆè·³è¿‡ç¼“å­˜æ¢æµ‹ï¼Œæ‰€æœ‰è‚¡ç¥¨ start_time=\"\"ï¼‰",
    )
    parser.add_argument(
        "--since",
        type=int,
        default=None,
        metavar="YYYY",
        help="æŒ‰å¹´åº¦åˆ†æ®µä¸‹è½½ï¼Œä»æŒ‡å®šå¹´ä»½å¼€å§‹ (å¦‚ --since 2025 ä¸‹è½½ 2025 è‡³ä»Š)",
    )
    parser.add_argument(
        "--periods",
        default="1d,5m,1m",
        help="K çº¿å‘¨æœŸï¼Œé€—å·åˆ†éš” (é»˜è®¤: 1d,5m,1m)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=20,
        help="è´¢åŠ¡æ•°æ®æ¯æ‰¹è‚¡ç¥¨æ•°é‡ (é»˜è®¤: 20ï¼ŒK çº¿é€åªä¸‹è½½ä¸å—æ­¤å‚æ•°å½±å“)",
    )
    parser.add_argument(
        "--tables",
        default="Balance,Income,CashFlow",
        help="è´¢åŠ¡æŠ¥è¡¨ç±»å‹ï¼Œé€—å·åˆ†éš” (é»˜è®¤: Balance,Income,CashFlow)",
    )
    parser.add_argument(
        "--skip-kline",
        action="store_true",
        help="è·³è¿‡ K çº¿ä¸‹è½½",
    )
    parser.add_argument(
        "--skip-financial",
        action="store_true",
        help="è·³è¿‡è´¢åŠ¡æ•°æ®ä¸‹è½½",
    )
    parser.add_argument(
        "--sectors",
        default=DEFAULT_SECTORS,
        help=f"ç›®æ ‡æ¿å—ï¼Œé€—å·åˆ†éš” (é»˜è®¤: {DEFAULT_SECTORS})",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=120,
        help="è´¢åŠ¡æ•°æ®æ¯æ‰¹ä¸‹è½½è¶…æ—¶ç§’æ•° (é»˜è®¤: 120ï¼ŒK çº¿è¶…æ—¶ç”± STOCK_TIMEOUT å¸¸é‡æ§åˆ¶)",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.2,
        help="è´¢åŠ¡æ•°æ®æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•° (é»˜è®¤: 0.2ï¼ŒK çº¿é€åªä¸‹è½½æ— å»¶è¿Ÿ)",
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=2,
        help="è¶…æ—¶æ‰¹æ¬¡æœ€å¤§è‡ªåŠ¨é‡è¯•æ¬¡æ•° (é»˜è®¤: 2)",
    )
    return parser.parse_args()


def print_summary(
    total: int,
    elapsed: float,
    kline_results: dict[str, dict[str, int]] | None,
    financial_result: dict[str, int] | None,
    full: bool,
    since_year: int | None = None,
    state_saved: bool = False,
) -> None:
    """æ‰“å°ä¸‹è½½ç»“æœæ±‡æ€»ï¼ˆå«æ¢æµ‹åˆ†ç»„ä¿¡æ¯ï¼‰ã€‚"""
    minutes = elapsed / 60
    has_failure = False

    print()
    print("=" * 60)
    print("ä¸‹è½½å®Œæˆ â€” ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"è‚¡ç¥¨æ€»æ•°: {total}")
    print(f"è€—æ—¶: {elapsed:.1f} ç§’ ({minutes:.1f} åˆ†é’Ÿ)")

    if kline_results:
        print()
        print("Kçº¿æ•°æ®:")
        for period, counts in kline_results.items():
            n_groups = counts.get("date_groups", 0)
            if since_year is not None:
                mode_info = f"å¹´åº¦åˆ†æ®µ (since {since_year}): {n_groups} ä¸ªå¹´åº¦ç»„"
            elif full:
                mode_info = "å…¨é‡"
            elif n_groups > 0:
                mode_info = f"ç²¾å‡†å¢é‡: {n_groups} ä¸ªæ—¥æœŸç»„"
            else:
                mode_info = ""
            if counts["fail"] == 0:
                print(f"  {period}: æˆåŠŸ {counts['ok']}, OK ({mode_info})")
            else:
                has_failure = True
                timeout_info = f" (è¶…æ—¶ {counts['timeout']})" if counts.get("timeout") else ""
                print(f"  {period}: æˆåŠŸ {counts['ok']}, å¤±è´¥ {counts['fail']}{timeout_info} ({mode_info})")

    if financial_result:
        print()
        if financial_result["fail"] == 0:
            print(f"è´¢åŠ¡æ•°æ®: æˆåŠŸ {financial_result['ok']}, OK")
        else:
            has_failure = True
            timeout_info = f" (è¶…æ—¶ {financial_result['timeout']})" if financial_result.get("timeout") else ""
            print(
                f"è´¢åŠ¡æ•°æ®: æˆåŠŸ {financial_result['ok']}, "
                f"å¤±è´¥ {financial_result['fail']}{timeout_info}"
            )

    if has_failure:
        print()
        print("âš ï¸  éƒ¨åˆ†æ‰¹æ¬¡ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—åé‡è¯•")

    if state_saved:
        print()
        print(f"çŠ¶æ€æ–‡ä»¶: {STATE_FILE}")

    print("=" * 60)


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    args = parse_args()

    logger.info("æ—¥å¿—æ–‡ä»¶: %s", _log_file)
    print(f"æ—¥å¿—æ–‡ä»¶: {_log_file}")

    # â”€â”€ çŠ¶æ€ç®¡ç† â”€â”€
    state = load_state()

    # 1. è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ”¯æŒå¤šæ¿å—åˆå¹¶å»é‡ï¼‰
    sectors = [s.strip() for s in args.sectors.split(",")]
    stocks: list[str] = []
    seen: set[str] = set()
    for sector in sectors:
        codes = xtdata.get_stock_list_in_sector(sector)
        logger.info("æ¿å— [%s] è¿”å› %d åª", sector, len(codes))
        for c in codes:
            if c not in seen:
                seen.add(c)
                stocks.append(c)
    if not stocks:
        logger.error("æ¿å— %s è¿”å›ç©ºåˆ—è¡¨", sectors)
        print(f"é”™è¯¯: æ¿å— {sectors} è¿”å›ç©ºåˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ xtdata è¿æ¥çŠ¶æ€")
        sys.exit(1)
    print(f"æ¿å— {sectors} å…± {len(stocks)} åªæ ‡çš„")

    periods = [p.strip() for p in args.periods.split(",")]
    tables = [t.strip() for t in args.tables.split(",")]

    # æ‰“å°æ¨¡å¼ä¿¡æ¯
    if args.since is not None and args.full:
        print(f"æ¨¡å¼: å¹´åº¦å¼ºåˆ¶å…¨é‡ä¸‹è½½ (--since {args.since} --full)")
        logger.info("å¹´åº¦å¼ºåˆ¶å…¨é‡æ¨¡å¼ (--since %d --full)", args.since)
    elif args.since is not None:
        print(f"æ¨¡å¼: å¹´åº¦åˆ†æ®µä¸‹è½½ (--since {args.since}ï¼Œè‡ªåŠ¨è·³è¿‡å·²ç¼“å­˜å¹´ä»½)")
        logger.info("å¹´åº¦åˆ†æ®µä¸‹è½½æ¨¡å¼ (--since %d)", args.since)
    elif args.full:
        print("æ¨¡å¼: å¼ºåˆ¶å…¨é‡ä¸‹è½½ (--full)")
        logger.info("å¼ºåˆ¶å…¨é‡æ¨¡å¼ (--full)")
    else:
        print("æ¨¡å¼: é€è‚¡ç²¾å‡†å¢é‡ (åŸºäºæœ¬åœ°ç¼“å­˜æ¢æµ‹)")
        logger.info("é€è‚¡ç²¾å‡†å¢é‡æ¨¡å¼")

    logger.info("è¶…æ—¶: %dç§’/æ‰¹, å»¶è¿Ÿ: %.1fç§’/æ‰¹, æœ€å¤§é‡è¯•: %d", args.timeout, args.delay, args.max_retries)
    print(f"è¶…æ—¶: {args.timeout}ç§’/æ‰¹, æ‰¹æ¬¡é—´å»¶è¿Ÿ: {args.delay}ç§’, å¤±è´¥è‡ªåŠ¨é‡è¯•: {args.max_retries} è½®")

    print()
    t0 = time.time()
    kline_results = None
    financial_result = None
    today = datetime.now().strftime("%Y%m%d")
    now_iso = datetime.now().isoformat(timespec="seconds")

    try:
        # 2. K çº¿ä¸‹è½½
        if not args.skip_kline:
            print(f"å¼€å§‹ä¸‹è½½ K çº¿æ•°æ® (å‘¨æœŸ: {', '.join(periods)})...")
            kline_results = download_kline_v2(
                stocks, periods,
                full=args.full,
                max_retries=args.max_retries,
                since_year=args.since,
            )
        else:
            print("è·³è¿‡ K çº¿ä¸‹è½½")

        # 3. è´¢åŠ¡æ•°æ®ä¸‹è½½
        if not args.skip_financial:
            print(f"\nå¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ® (æŠ¥è¡¨: {', '.join(tables)})...")
            financial_result = download_financial(
                stocks, tables, args.batch_size,
                timeout=args.timeout, delay=args.delay, max_retries=args.max_retries,
                full=args.full,
            )
        else:
            print("è·³è¿‡è´¢åŠ¡æ•°æ®ä¸‹è½½")
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        print("\n\nç”¨æˆ·ä¸­æ–­ (Ctrl+C)")

    elapsed = time.time() - t0

    # â”€â”€ æ›´æ–°çŠ¶æ€ â”€â”€
    if kline_results:
        for period, counts in kline_results.items():
            task_key = f"kline:{period}"
            old = state.tasks.get(task_key)
            ts = TaskState(
                last_success_date=old.last_success_date if old else "",
                last_run_iso=now_iso,
                stock_count=len(stocks),
                ok=counts["ok"],
                fail=counts["fail"],
            )
            if counts["fail"] == 0:
                ts.last_success_date = today
            state.tasks[task_key] = ts
    if financial_result:
        task_key = "financial"
        old = state.tasks.get(task_key)
        ts = TaskState(
            last_success_date=old.last_success_date if old else "",
            last_run_iso=now_iso,
            stock_count=len(stocks),
            ok=financial_result["ok"],
            fail=financial_result["fail"],
        )
        if financial_result["fail"] == 0:
            ts.last_success_date = today
        state.tasks[task_key] = ts
    save_state(state)

    # 4. æ±‡æ€»ï¼ˆå³ä½¿ä¸­æ–­ä¹Ÿæ‰“å°å·²å®Œæˆçš„éƒ¨åˆ†ï¼‰
    print_summary(
        len(stocks), elapsed, kline_results, financial_result,
        full=args.full, since_year=args.since, state_saved=True,
    )
    logger.info("å®Œæˆï¼Œè€—æ—¶ %.1f ç§’", elapsed)

    # xtdata ä¸‹è½½çº¿ç¨‹æ˜¯é daemon çº¿ç¨‹ï¼Œä¸­æ–­åä»åœ¨åå°è¿è¡Œï¼Œ
    # æ­£å¸¸ sys.exit() ä¼šç­‰å¾…è¿™äº›çº¿ç¨‹å®Œæˆå¯¼è‡´å¡æ­»ï¼Œéœ€å¼ºåˆ¶é€€å‡ºã€‚
    if _interrupted:
        os._exit(0)


if __name__ == "__main__":
    main()
